{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Import and Overview the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read a CSV file, inspect, and return a DataFrame copy\n",
    "def read_and_inspect_csv(file_path):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df_original = pd.read_csv(file_path)\n",
    "\n",
    "    # Create a copy of the DataFrame\n",
    "    df_copy = df_original.copy()\n",
    "\n",
    "    # Print some details about the DataFrame for quick inspection\n",
    "    print(f\"\\nInspection results for {file_path.split('/')[-1]}:\")\n",
    "    print(df_copy.head(10))  # First 10 rows\n",
    "    print(\"*\"*30)\n",
    "    print(\"\\nInfo:\")\n",
    "    df_copy.info()\n",
    "    print(\"*\"*30)\n",
    "    print(\"\\nStatistics:\")\n",
    "    print(df_copy.describe())\n",
    "    print(\"*\"*30)\n",
    "    print(\"\\nNumber of duplicates:\")\n",
    "    print(df_copy.duplicated().sum())\n",
    "\n",
    "    # Return the copied DataFrame for further use\n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspection results for D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Origin Data\\customer.csv:\n",
      "   customer_id  home_store customer_first-name        customer_email  \\\n",
      "0            1           3           Kelly Key  Venus@adipiscing.edu   \n",
      "1            2           3     Clark Schroeder        Nora@fames.gov   \n",
      "2            3           3      Elvis Cardenas    Brianna@tellus.edu   \n",
      "3            4           3        Rafael Estes           Ina@non.gov   \n",
      "4            5           3          Colin Lynn      Dale@Integer.com   \n",
      "5            6           3          Igor Beach       Caleb@morbi.net   \n",
      "6            7           3        Scott Holden       Yen@Integer.edu   \n",
      "7            8           3        Keegan Ayala       Tana@sociis.com   \n",
      "8            9           3          Amir Byers  Madeson@malesuada.us   \n",
      "9           10           3        Magee Malone    Anjolie@sapien.gov   \n",
      "\n",
      "  customer_since loyalty_card_number   birthdate gender  birth_year  \n",
      "0     2017-01-04        908-424-2890  1950-05-29      M        1950  \n",
      "1     2017-01-07        032-732-6308  1950-07-30      M        1950  \n",
      "2     2017-01-10        459-375-9187  1950-09-30      M        1950  \n",
      "3     2017-01-13        576-640-9226  1950-12-01      M        1950  \n",
      "4     2017-01-15        344-674-6569  1951-02-01      M        1951  \n",
      "5     2017-01-18        114-126-1992  1951-04-04      M        1951  \n",
      "6     2017-01-21        384-074-3606  1951-06-05      M        1951  \n",
      "7     2017-01-24        257-308-7675  1951-08-07      M        1951  \n",
      "8     2017-01-26        931-925-0273  1951-10-08      M        1951  \n",
      "9     2017-01-29        359-150-6747  1951-12-09      M        1951  \n",
      "******************************\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2246 entries, 0 to 2245\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   customer_id          2246 non-null   int64 \n",
      " 1   home_store           2246 non-null   int64 \n",
      " 2   customer_first-name  2246 non-null   object\n",
      " 3   customer_email       2246 non-null   object\n",
      " 4   customer_since       2246 non-null   object\n",
      " 5   loyalty_card_number  2246 non-null   object\n",
      " 6   birthdate            2246 non-null   object\n",
      " 7   gender               2246 non-null   object\n",
      " 8   birth_year           2246 non-null   int64 \n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 158.1+ KB\n",
      "******************************\n",
      "\n",
      "Statistics:\n",
      "       customer_id   home_store   birth_year\n",
      "count  2246.000000  2246.000000  2246.000000\n",
      "mean   4285.902048     4.956812  1978.385574\n",
      "std    3088.088265     1.852562    14.925503\n",
      "min       1.000000     3.000000  1950.000000\n",
      "25%     562.250000     3.000000  1965.000000\n",
      "50%    5323.500000     5.000000  1981.000000\n",
      "75%    5884.750000     5.000000  1991.000000\n",
      "max    8501.000000     8.000000  2001.000000\n",
      "******************************\n",
      "\n",
      "Number of duplicates:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df_customer = read_and_inspect_csv(r\"D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Origin Data\\customer.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspection results for D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Origin Data\\Dates.csv:\n",
      "  transaction_date   Date_ID  Week_ID Week_Desc  Month_ID Month_Name  \\\n",
      "0         4/1/2019  20190401       14   Week 14         4      April   \n",
      "1         4/2/2019  20190402       14   Week 14         4      April   \n",
      "2         4/3/2019  20190403       14   Week 14         4      April   \n",
      "3         4/4/2019  20190404       14   Week 14         4      April   \n",
      "4         4/5/2019  20190405       14   Week 14         4      April   \n",
      "5         4/6/2019  20190406       14   Week 14         4      April   \n",
      "6         4/7/2019  20190407       14   Week 14         4      April   \n",
      "7         4/8/2019  20190408       15   Week 15         4      April   \n",
      "8         4/9/2019  20190409       15   Week 15         4      April   \n",
      "9        4/10/2019  20190410       15   Week 15         4      April   \n",
      "\n",
      "   Quarter_ID Quarter_Name  Year_ID  \n",
      "0           2           Q2     2019  \n",
      "1           2           Q2     2019  \n",
      "2           2           Q2     2019  \n",
      "3           2           Q2     2019  \n",
      "4           2           Q2     2019  \n",
      "5           2           Q2     2019  \n",
      "6           2           Q2     2019  \n",
      "7           2           Q2     2019  \n",
      "8           2           Q2     2019  \n",
      "9           2           Q2     2019  \n",
      "******************************\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30 entries, 0 to 29\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   transaction_date  30 non-null     object\n",
      " 1   Date_ID           30 non-null     int64 \n",
      " 2   Week_ID           30 non-null     int64 \n",
      " 3   Week_Desc         30 non-null     object\n",
      " 4   Month_ID          30 non-null     int64 \n",
      " 5   Month_Name        30 non-null     object\n",
      " 6   Quarter_ID        30 non-null     int64 \n",
      " 7   Quarter_Name      30 non-null     object\n",
      " 8   Year_ID           30 non-null     int64 \n",
      "dtypes: int64(5), object(4)\n",
      "memory usage: 2.2+ KB\n",
      "******************************\n",
      "\n",
      "Statistics:\n",
      "            Date_ID    Week_ID  Month_ID  Quarter_ID  Year_ID\n",
      "count  3.000000e+01  30.000000      30.0        30.0     30.0\n",
      "mean   2.019042e+07  15.666667       4.0         2.0   2019.0\n",
      "std    8.803408e+00   1.268541       0.0         0.0      0.0\n",
      "min    2.019040e+07  14.000000       4.0         2.0   2019.0\n",
      "25%    2.019041e+07  15.000000       4.0         2.0   2019.0\n",
      "50%    2.019042e+07  16.000000       4.0         2.0   2019.0\n",
      "75%    2.019042e+07  17.000000       4.0         2.0   2019.0\n",
      "max    2.019043e+07  18.000000       4.0         2.0   2019.0\n",
      "******************************\n",
      "\n",
      "Number of duplicates:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df_dates = read_and_inspect_csv(r\"D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Origin Data\\Dates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspection results for D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Origin Data\\generations.csv:\n",
      "   birth_year    generation\n",
      "0        1946  Baby Boomers\n",
      "1        1947  Baby Boomers\n",
      "2        1948  Baby Boomers\n",
      "3        1949  Baby Boomers\n",
      "4        1950  Baby Boomers\n",
      "5        1951  Baby Boomers\n",
      "6        1952  Baby Boomers\n",
      "7        1953  Baby Boomers\n",
      "8        1954  Baby Boomers\n",
      "9        1955  Baby Boomers\n",
      "******************************\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70 entries, 0 to 69\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   birth_year  70 non-null     int64 \n",
      " 1   generation  70 non-null     object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.2+ KB\n",
      "******************************\n",
      "\n",
      "Statistics:\n",
      "        birth_year\n",
      "count    70.000000\n",
      "mean   1980.500000\n",
      "std      20.351085\n",
      "min    1946.000000\n",
      "25%    1963.250000\n",
      "50%    1980.500000\n",
      "75%    1997.750000\n",
      "max    2015.000000\n",
      "******************************\n",
      "\n",
      "Number of duplicates:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df_generations = read_and_inspect_csv(r\"D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Origin Data\\generations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspection results for D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Origin Data\\pastry inventory.csv:\n",
      "   sales_outlet_id transaction_date  product_id  start_of_day  quantity_sold  \\\n",
      "0                3         4/1/2019          69            18              8   \n",
      "1                3         4/1/2019          70            18             12   \n",
      "2                3         4/1/2019          71            18              8   \n",
      "3                3         4/1/2019          72            48              9   \n",
      "4                3         4/1/2019          73            18              9   \n",
      "5                3         4/2/2019          69            18              7   \n",
      "6                3         4/2/2019          70            18             10   \n",
      "7                3         4/2/2019          71            18             10   \n",
      "8                3         4/2/2019          72            48             10   \n",
      "9                3         4/2/2019          73            18              9   \n",
      "\n",
      "   waste % waste  \n",
      "0     10     56%  \n",
      "1      6     33%  \n",
      "2     10     56%  \n",
      "3     39     81%  \n",
      "4      9     50%  \n",
      "5     11     61%  \n",
      "6      8     44%  \n",
      "7      8     44%  \n",
      "8     38     79%  \n",
      "9      9     50%  \n",
      "******************************\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 307 entries, 0 to 306\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   sales_outlet_id   307 non-null    int64 \n",
      " 1   transaction_date  307 non-null    object\n",
      " 2   product_id        307 non-null    int64 \n",
      " 3   start_of_day      307 non-null    int64 \n",
      " 4   quantity_sold     307 non-null    int64 \n",
      " 5   waste             307 non-null    int64 \n",
      " 6   % waste           307 non-null    object\n",
      "dtypes: int64(5), object(2)\n",
      "memory usage: 16.9+ KB\n",
      "******************************\n",
      "\n",
      "Statistics:\n",
      "       sales_outlet_id  product_id  start_of_day  quantity_sold       waste\n",
      "count       307.000000  307.000000    307.000000     307.000000  307.000000\n",
      "mean          5.394137   70.983713     24.058632       9.296417   14.657980\n",
      "std           2.049477    1.417582     12.063414       5.440115   11.202108\n",
      "min           3.000000   69.000000     18.000000       0.000000    0.000000\n",
      "25%           3.000000   70.000000     18.000000       6.000000    8.000000\n",
      "50%           5.000000   71.000000     18.000000       8.000000   11.000000\n",
      "75%           8.000000   72.000000     18.000000      11.000000   15.000000\n",
      "max           8.000000   73.000000     48.000000      32.000000   47.000000\n",
      "******************************\n",
      "\n",
      "Number of duplicates:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df_pastry = read_and_inspect_csv(r\"D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Origin Data\\pastry inventory.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspection results for D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Origin Data\\product.csv:\n",
      "   product_id    product_group product_category       product_type  \\\n",
      "0           1  Whole Bean/Teas     Coffee beans      Organic Beans   \n",
      "1           2  Whole Bean/Teas     Coffee beans  House blend Beans   \n",
      "2           3  Whole Bean/Teas     Coffee beans     Espresso Beans   \n",
      "3           4  Whole Bean/Teas     Coffee beans     Espresso Beans   \n",
      "4           5  Whole Bean/Teas     Coffee beans      Gourmet Beans   \n",
      "5           6  Whole Bean/Teas     Coffee beans      Gourmet Beans   \n",
      "6           7  Whole Bean/Teas     Coffee beans      Premium Beans   \n",
      "7           8  Whole Bean/Teas     Coffee beans      Premium Beans   \n",
      "8           9  Whole Bean/Teas     Coffee beans      Organic Beans   \n",
      "9          10  Whole Bean/Teas     Coffee beans        Green beans   \n",
      "\n",
      "                        product  \\\n",
      "0           Brazilian - Organic   \n",
      "1      Our Old Time Diner Blend   \n",
      "2                Espresso Roast   \n",
      "3          Primo Espresso Roast   \n",
      "4        Columbian Medium Roast   \n",
      "5                      Ethiopia   \n",
      "6         Jamacian Coffee River   \n",
      "7                     Civet Cat   \n",
      "8           Organic Decaf Blend   \n",
      "9  Guatemalan Sustainably Grown   \n",
      "\n",
      "                                 product_description unit_of_measure  \\\n",
      "0     It's like Carnival in a cup. Clean and smooth.           12 oz   \n",
      "1  Out packed blend of beans that is reminiscent ...           12 oz   \n",
      "2          Our house blend for a good espresso shot.            1 lb   \n",
      "3   Our primium single source of hand roasted beans.            1 lb   \n",
      "4           A smooth cup of coffee any time of day.             1 lb   \n",
      "5                           From the home of coffee.            1 lb   \n",
      "6         Ya man, it will start your day off right.             1 lb   \n",
      "7  The most expensive coffee in the world, the ca...           .5 lb   \n",
      "8  Our blend of hand picked organic beans that ha...            1 lb   \n",
      "9               Green beans you can roast yourself.             1 lb   \n",
      "\n",
      "   current_wholesale_price current_retail_price tax_exempt_yn promo_yn  \\\n",
      "0                    14.40              $18.00              Y        N   \n",
      "1                    14.40              $18.00              Y        N   \n",
      "2                    11.80              $14.75              Y        N   \n",
      "3                    16.36              $20.45              Y        N   \n",
      "4                    12.00              $15.00              Y        N   \n",
      "5                    16.80              $21.00              Y        N   \n",
      "6                    15.80              $19.75              Y        N   \n",
      "7                    36.00              $45.00              Y        N   \n",
      "8                    18.00              $22.50              Y        N   \n",
      "9                     8.00              $10.00              Y        N   \n",
      "\n",
      "  new_product_yn  \n",
      "0              N  \n",
      "1              N  \n",
      "2              N  \n",
      "3              N  \n",
      "4              N  \n",
      "5              N  \n",
      "6              N  \n",
      "7              N  \n",
      "8              N  \n",
      "9              N  \n",
      "******************************\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 88 entries, 0 to 87\n",
      "Data columns (total 12 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   product_id               88 non-null     int64  \n",
      " 1   product_group            88 non-null     object \n",
      " 2   product_category         88 non-null     object \n",
      " 3   product_type             88 non-null     object \n",
      " 4   product                  88 non-null     object \n",
      " 5   product_description      88 non-null     object \n",
      " 6   unit_of_measure          88 non-null     object \n",
      " 7   current_wholesale_price  88 non-null     float64\n",
      " 8   current_retail_price     88 non-null     object \n",
      " 9   tax_exempt_yn            88 non-null     object \n",
      " 10  promo_yn                 88 non-null     object \n",
      " 11  new_product_yn           88 non-null     object \n",
      "dtypes: float64(1), int64(1), object(10)\n",
      "memory usage: 8.4+ KB\n",
      "******************************\n",
      "\n",
      "Statistics:\n",
      "       product_id  current_wholesale_price\n",
      "count   88.000000                88.000000\n",
      "mean    44.534091                 3.888409\n",
      "std     25.605293                 5.649682\n",
      "min      1.000000                 0.040000\n",
      "25%     22.750000                 0.630000\n",
      "50%     44.500000                 1.195000\n",
      "75%     66.250000                 5.360000\n",
      "max     89.000000                36.000000\n",
      "******************************\n",
      "\n",
      "Number of duplicates:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df_product = read_and_inspect_csv(r\"D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Origin Data\\product.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspection results for D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Origin Data\\sales targets.csv:\n",
      "   sales_outlet_id year_month  beans_goal  beverage_goal  food_goal  \\\n",
      "0                3     Apr-19         720          13500       3420   \n",
      "1                4     Apr-19         720          13500       3420   \n",
      "2                5     Apr-19        1000          18750       4750   \n",
      "3                6     Apr-19         720          13500       3420   \n",
      "4                7     Apr-19         720          13500       3420   \n",
      "5                8     Apr-19         900          16875       4275   \n",
      "6                9     Apr-19         720          13500       3420   \n",
      "7               10     Apr-19         720          13500       3420   \n",
      "\n",
      "   merchandise _goal  total_goal  \n",
      "0                360       18000  \n",
      "1                360       18000  \n",
      "2                500       25000  \n",
      "3                360       18000  \n",
      "4                360       18000  \n",
      "5                450       22500  \n",
      "6                360       18000  \n",
      "7                360       18000  \n",
      "******************************\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8 entries, 0 to 7\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   sales_outlet_id    8 non-null      int64 \n",
      " 1   year_month         8 non-null      object\n",
      " 2   beans_goal         8 non-null      int64 \n",
      " 3   beverage_goal      8 non-null      int64 \n",
      " 4   food_goal          8 non-null      int64 \n",
      " 5   merchandise _goal  8 non-null      int64 \n",
      " 6   total_goal         8 non-null      int64 \n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 580.0+ bytes\n",
      "******************************\n",
      "\n",
      "Statistics:\n",
      "       sales_outlet_id   beans_goal  beverage_goal    food_goal  \\\n",
      "count          8.00000     8.000000       8.000000     8.000000   \n",
      "mean           6.50000   777.500000   14578.125000  3693.125000   \n",
      "std            2.44949   109.772492    2058.234225   521.419337   \n",
      "min            3.00000   720.000000   13500.000000  3420.000000   \n",
      "25%            4.75000   720.000000   13500.000000  3420.000000   \n",
      "50%            6.50000   720.000000   13500.000000  3420.000000   \n",
      "75%            8.25000   765.000000   14343.750000  3633.750000   \n",
      "max           10.00000  1000.000000   18750.000000  4750.000000   \n",
      "\n",
      "       merchandise _goal  total_goal  \n",
      "count           8.000000      8.0000  \n",
      "mean          388.750000  19437.5000  \n",
      "std            54.886246   2744.3123  \n",
      "min           360.000000  18000.0000  \n",
      "25%           360.000000  18000.0000  \n",
      "50%           360.000000  18000.0000  \n",
      "75%           382.500000  19125.0000  \n",
      "max           500.000000  25000.0000  \n",
      "******************************\n",
      "\n",
      "Number of duplicates:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df_sales_targets = read_and_inspect_csv(r\"D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Origin Data\\sales targets.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspection results for D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Origin Data\\sales_outlet.csv:\n",
      "   sales_outlet_id sales_outlet_type  store_square_feet       store_address  \\\n",
      "0                2         warehouse               3400  164-14 Jamaica Ave   \n",
      "1                3            retail               1300      32-20 Broadway   \n",
      "2                4            retail               1300    604 Union Street   \n",
      "3                5            retail                900   100 Church Street   \n",
      "4                6            retail               1000      122 E Broadway   \n",
      "5                7            retail               1200   224 E 57th Street   \n",
      "6                8            retail               1500      687 9th Avenue   \n",
      "7                9            retail               1700      175 8th Avenue   \n",
      "8               10            retail               1600   183 W 10th Street   \n",
      "\n",
      "         store_city store_state_province store_telephone  store_postal_code  \\\n",
      "0           Jamaica                   NY    972-871-0402              11432   \n",
      "1  Long Island City                   NY    777-718-3190              11106   \n",
      "2          Brooklyn                   NY    619-347-5193              11215   \n",
      "3          New York                   NY    343-212-5151              10007   \n",
      "4          New York                   NY    613-555-4989              10002   \n",
      "5          New York                   NY    287-817-2330              10021   \n",
      "6          New York                   NY    652-212-7020              10036   \n",
      "7          New York                   NY    242-212-0080              10011   \n",
      "8          New York                   NY    674-646-6434              10014   \n",
      "\n",
      "   store_longitude  store_latitude  manager        Neighorhood  \n",
      "0       -73.795168       40.705226      NaN            Jamaica  \n",
      "1       -73.924008       40.761196      6.0            Astoria  \n",
      "2       -73.983984       40.677645     11.0            Gowanus  \n",
      "3       -74.010130       40.713290     16.0    Lower Manhattan  \n",
      "4       -73.992687       40.713852     21.0    Lower East Side  \n",
      "5       -73.960000       40.770000     26.0    Upper East Side  \n",
      "6       -73.990338       40.761887     31.0     Hell's Kitchen  \n",
      "7       -74.000502       40.742760     36.0            Chelsea  \n",
      "8       -74.002722       40.734367     41.0  Greenwich Village  \n",
      "******************************\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9 entries, 0 to 8\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   sales_outlet_id       9 non-null      int64  \n",
      " 1   sales_outlet_type     9 non-null      object \n",
      " 2   store_square_feet     9 non-null      int64  \n",
      " 3   store_address         9 non-null      object \n",
      " 4   store_city            9 non-null      object \n",
      " 5   store_state_province  9 non-null      object \n",
      " 6   store_telephone       9 non-null      object \n",
      " 7   store_postal_code     9 non-null      int64  \n",
      " 8   store_longitude       9 non-null      float64\n",
      " 9   store_latitude        9 non-null      float64\n",
      " 10  manager               8 non-null      float64\n",
      " 11  Neighorhood           9 non-null      object \n",
      "dtypes: float64(3), int64(3), object(6)\n",
      "memory usage: 996.0+ bytes\n",
      "******************************\n",
      "\n",
      "Statistics:\n",
      "       sales_outlet_id  store_square_feet  store_postal_code  store_longitude  \\\n",
      "count         9.000000           9.000000           9.000000         9.000000   \n",
      "mean          6.000000        1544.444444       10427.111111       -73.962171   \n",
      "std           2.738613         743.490267         623.535974         0.067975   \n",
      "min           2.000000         900.000000       10002.000000       -74.010130   \n",
      "25%           4.000000        1200.000000       10011.000000       -74.000502   \n",
      "50%           6.000000        1300.000000       10021.000000       -73.990338   \n",
      "75%           8.000000        1600.000000       11106.000000       -73.960000   \n",
      "max          10.000000        3400.000000       11432.000000       -73.795168   \n",
      "\n",
      "       store_latitude    manager  \n",
      "count        9.000000   8.000000  \n",
      "mean        40.731136  23.500000  \n",
      "std          0.030933  12.247449  \n",
      "min         40.677645   6.000000  \n",
      "25%         40.713290  14.750000  \n",
      "50%         40.734367  23.500000  \n",
      "75%         40.761196  32.250000  \n",
      "max         40.770000  41.000000  \n",
      "******************************\n",
      "\n",
      "Number of duplicates:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df_sales_outlet = read_and_inspect_csv(r\"D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Origin Data\\sales_outlet.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspection results for D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Origin Data\\staff.csv:\n",
      "   staff_id first_name last_name         position  start_date location  \\\n",
      "0         1        Sue   Tindale              CFO    8/3/2001       HQ   \n",
      "1         2        Ian   Tindale              CEO    8/3/2001       HQ   \n",
      "2         3      Marny  Hermione          Roaster  10/24/2007       WH   \n",
      "3         4    Chelsea   Claudia          Roaster    7/3/2003       WH   \n",
      "4         5       Alec   Isadora          Roaster    4/2/2008       WH   \n",
      "5         6       Xena     Rahim    Store Manager   7/24/2016        3   \n",
      "6         7     Kelsey   Cameron  Coffee Wrangler  10/18/2003        3   \n",
      "7         8   Hamilton       Emi  Coffee Wrangler    2/9/2005        3   \n",
      "8         9   Caldwell      Veda  Coffee Wrangler    9/9/2013        3   \n",
      "9        10        Ima  Winifred  Coffee Wrangler  12/10/2016        3   \n",
      "\n",
      "   Unnamed: 6  Unnamed: 7  \n",
      "0         NaN         NaN  \n",
      "1         NaN         NaN  \n",
      "2         NaN         NaN  \n",
      "3         NaN         NaN  \n",
      "4         NaN         NaN  \n",
      "5         NaN         NaN  \n",
      "6         NaN         NaN  \n",
      "7         NaN         NaN  \n",
      "8         NaN         NaN  \n",
      "9         NaN         NaN  \n",
      "******************************\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55 entries, 0 to 54\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   staff_id    55 non-null     int64  \n",
      " 1   first_name  55 non-null     object \n",
      " 2   last_name   55 non-null     object \n",
      " 3   position    55 non-null     object \n",
      " 4   start_date  55 non-null     object \n",
      " 5   location    55 non-null     object \n",
      " 6   Unnamed: 6  0 non-null      float64\n",
      " 7   Unnamed: 7  0 non-null      float64\n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 3.6+ KB\n",
      "******************************\n",
      "\n",
      "Statistics:\n",
      "       staff_id  Unnamed: 6  Unnamed: 7\n",
      "count  55.00000         0.0         0.0\n",
      "mean   28.00000         NaN         NaN\n",
      "std    16.02082         NaN         NaN\n",
      "min     1.00000         NaN         NaN\n",
      "25%    14.50000         NaN         NaN\n",
      "50%    28.00000         NaN         NaN\n",
      "75%    41.50000         NaN         NaN\n",
      "max    55.00000         NaN         NaN\n",
      "******************************\n",
      "\n",
      "Number of duplicates:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df_staff = read_and_inspect_csv(r\"D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Origin Data\\staff.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 df_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of the 'customer_since', 'birthdate' column to datetime\n",
    "df_customer['customer_since'] = pd.to_datetime(df_customer['customer_since'])\n",
    "df_customer['birthdate'] = pd.to_datetime(df_customer['birthdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>home_store</th>\n",
       "      <th>customer_first-name</th>\n",
       "      <th>customer_email</th>\n",
       "      <th>customer_since</th>\n",
       "      <th>loyalty_card_number</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>gender</th>\n",
       "      <th>birth_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly Key</td>\n",
       "      <td>Venus@adipiscing.edu</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>908-424-2890</td>\n",
       "      <td>1950-05-29</td>\n",
       "      <td>M</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Clark Schroeder</td>\n",
       "      <td>Nora@fames.gov</td>\n",
       "      <td>2017-01-07</td>\n",
       "      <td>032-732-6308</td>\n",
       "      <td>1950-07-30</td>\n",
       "      <td>M</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Elvis Cardenas</td>\n",
       "      <td>Brianna@tellus.edu</td>\n",
       "      <td>2017-01-10</td>\n",
       "      <td>459-375-9187</td>\n",
       "      <td>1950-09-30</td>\n",
       "      <td>M</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Rafael Estes</td>\n",
       "      <td>Ina@non.gov</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>576-640-9226</td>\n",
       "      <td>1950-12-01</td>\n",
       "      <td>M</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Colin Lynn</td>\n",
       "      <td>Dale@Integer.com</td>\n",
       "      <td>2017-01-15</td>\n",
       "      <td>344-674-6569</td>\n",
       "      <td>1951-02-01</td>\n",
       "      <td>M</td>\n",
       "      <td>1951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  home_store customer_first-name        customer_email  \\\n",
       "0            1           3           Kelly Key  Venus@adipiscing.edu   \n",
       "1            2           3     Clark Schroeder        Nora@fames.gov   \n",
       "2            3           3      Elvis Cardenas    Brianna@tellus.edu   \n",
       "3            4           3        Rafael Estes           Ina@non.gov   \n",
       "4            5           3          Colin Lynn      Dale@Integer.com   \n",
       "\n",
       "  customer_since loyalty_card_number  birthdate gender  birth_year  \n",
       "0     2017-01-04        908-424-2890 1950-05-29      M        1950  \n",
       "1     2017-01-07        032-732-6308 1950-07-30      M        1950  \n",
       "2     2017-01-10        459-375-9187 1950-09-30      M        1950  \n",
       "3     2017-01-13        576-640-9226 1950-12-01      M        1950  \n",
       "4     2017-01-15        344-674-6569 1951-02-01      M        1951  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_customer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 df_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of the 'transactions_date' column to datetime\n",
    "df_dates['transaction_date'] = pd.to_datetime(df_dates['transaction_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 df_pastry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of the 'transactions_date' column to datetime\n",
    "df_pastry['transaction_date'] = pd.to_datetime(df_pastry['transaction_date'])\n",
    "\n",
    "# Change the data type of the '% waste' column to int64\n",
    "df_pastry['% waste'] = df_pastry['% waste'].str.rstrip('%').astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 df_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of the 'current_price' column to float64 and add a new column measure in USD\n",
    "df_product['current_retail_price'] = df_product['current_retail_price'].str.lstrip('$').astype('float64')\n",
    "df_product['metric'] = df_product['current_retail_price'].apply(lambda x: f\"{x:.2f} USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tax_exempt_yn\n",
      "Y    72\n",
      "N    16\n",
      "Name: count, dtype: int64\n",
      "promo_yn\n",
      "N    84\n",
      "Y     4\n",
      "Name: count, dtype: int64\n",
      "new_product_yn\n",
      "N    86\n",
      "Y     2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "columns = df_product[['tax_exempt_yn', 'promo_yn', 'new_product_yn']]\n",
    "for cols in columns:\n",
    "    print(pd.Series(df_product[cols]).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 df_sales_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 'year_month' into 'year' and 'month' columns\n",
    "def extract_month_year(date_str):\n",
    "    # Parse using datetime, assuming the format 'MMM-YY'\n",
    "    parsed_date = datetime.strptime(date_str, '%b-%y')\n",
    "    \n",
    "    # Extract month as an integer\n",
    "    month = parsed_date.month\n",
    "    # Extract year (adding 2000 to handle two-digit years correctly)\n",
    "    year = parsed_date.year\n",
    "\n",
    "    return month, year\n",
    "\n",
    "# Apply the function to each date string and create new columns\n",
    "df_sales_targets[['Date_ID', 'Year_ID']] = df_sales_targets['year_month'].apply(lambda x: pd.Series(extract_month_year(x)))\n",
    "\n",
    "df_sales_targets.drop('year_month', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales_outlet_id</th>\n",
       "      <th>beans_goal</th>\n",
       "      <th>beverage_goal</th>\n",
       "      <th>food_goal</th>\n",
       "      <th>merchandise _goal</th>\n",
       "      <th>total_goal</th>\n",
       "      <th>Date_ID</th>\n",
       "      <th>Year_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>720</td>\n",
       "      <td>13500</td>\n",
       "      <td>3420</td>\n",
       "      <td>360</td>\n",
       "      <td>18000</td>\n",
       "      <td>4</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>720</td>\n",
       "      <td>13500</td>\n",
       "      <td>3420</td>\n",
       "      <td>360</td>\n",
       "      <td>18000</td>\n",
       "      <td>4</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>18750</td>\n",
       "      <td>4750</td>\n",
       "      <td>500</td>\n",
       "      <td>25000</td>\n",
       "      <td>4</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>720</td>\n",
       "      <td>13500</td>\n",
       "      <td>3420</td>\n",
       "      <td>360</td>\n",
       "      <td>18000</td>\n",
       "      <td>4</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>720</td>\n",
       "      <td>13500</td>\n",
       "      <td>3420</td>\n",
       "      <td>360</td>\n",
       "      <td>18000</td>\n",
       "      <td>4</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sales_outlet_id  beans_goal  beverage_goal  food_goal  merchandise _goal  \\\n",
       "0                3         720          13500       3420                360   \n",
       "1                4         720          13500       3420                360   \n",
       "2                5        1000          18750       4750                500   \n",
       "3                6         720          13500       3420                360   \n",
       "4                7         720          13500       3420                360   \n",
       "\n",
       "   total_goal  Date_ID  Year_ID  \n",
       "0       18000        4     2019  \n",
       "1       18000        4     2019  \n",
       "2       25000        4     2019  \n",
       "3       18000        4     2019  \n",
       "4       18000        4     2019  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 df_staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 2 useless columns\n",
    "df_staff.drop(['Unnamed: 6', 'Unnamed: 7'], axis=1, inplace=True)\n",
    "\n",
    "# Change the data type of the 'start_date' column to datetime\n",
    "df_staff['start_date'] = pd.to_datetime(df_staff['start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55 entries, 0 to 54\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   staff_id    55 non-null     int64         \n",
      " 1   first_name  55 non-null     object        \n",
      " 2   last_name   55 non-null     object        \n",
      " 3   position    55 non-null     object        \n",
      " 4   start_date  55 non-null     datetime64[ns]\n",
      " 5   location    55 non-null     object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(4)\n",
      "memory usage: 2.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_staff.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to .csv files in D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Preprocessing Data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def save_dataframes_to_csv_files(dataframes_dict, output_dir):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Loop through the DataFrame dictionary\n",
    "    for table_name, df in dataframes_dict.items():\n",
    "        # Define the output .csv file path\n",
    "        csv_file_path = os.path.join(output_dir, f\"{table_name}.csv\")\n",
    "        \n",
    "        # Write the DataFrame to a CSV file\n",
    "        df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "    print(f\"Data successfully saved to .csv files in {output_dir}\")\n",
    "\n",
    "# Example of your preprocessed dataframes\n",
    "dataframes_dict = {\n",
    "    'staff': df_staff,\n",
    "    'sales_outlet': df_sales_outlet,\n",
    "    'sales_targets': df_sales_targets,\n",
    "    'product': df_product,\n",
    "    'pastry_inventory': df_pastry,\n",
    "    'generations': df_generations,\n",
    "    'dates': df_dates,\n",
    "    'customer': df_customer\n",
    "}\n",
    "\n",
    "# Specify the output directory\n",
    "output_dir = r\"D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Preprocessing Data\"\n",
    "\n",
    "# Call the function to save each DataFrame as its own .csv file\n",
    "save_dataframes_to_csv_files(dataframes_dict, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\GitHub\\\\UEL-Course\\\\MLBA\\\\final_project\\\\Data\\\\Preprocessing Data\\\\staff.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m sqlite_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGitHub\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUEL-Course\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMLBA\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfinal_project\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdatabase.sqlite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Call the function to convert each CSV to a table in the SQLite database\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[43mcsv_to_sqlite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframes_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msqlite_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[95], line 16\u001b[0m, in \u001b[0;36mcsv_to_sqlite\u001b[1;34m(dataframes_dict, output_dir, sqlite_file)\u001b[0m\n\u001b[0;32m     13\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Read the CSV file into a DataFrame\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Write the DataFrame to the SQLite database\u001b[39;00m\n\u001b[0;32m     19\u001b[0m df\u001b[38;5;241m.\u001b[39mto_sql(table_name, conn, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\GitHub\\\\UEL-Course\\\\MLBA\\\\final_project\\\\Data\\\\Preprocessing Data\\\\staff.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "\n",
    "def csv_to_sqlite(dataframes_dict, output_dir, sqlite_file):\n",
    "    # Connect to SQLite database\n",
    "    conn = sqlite3.connect(sqlite_file)\n",
    "    \n",
    "    # Loop through the DataFrame dictionary\n",
    "    for table_name, csv_file in dataframes_dict.items():\n",
    "        # Define the CSV file path\n",
    "        csv_file_path = os.path.join(output_dir, f\"{csv_file}.csv\")\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # Write the DataFrame to the SQLite database\n",
    "        df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    \n",
    "    # Close the SQLite connection\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Data successfully converted to SQLite database at {sqlite_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# Dictionary mapping table names to CSV filenames\n",
    "dataframes_dict = {\n",
    "    'staff': 'staff',\n",
    "    'sales_outlet': 'sales_outlet',\n",
    "    'sales_targets': 'sales_targets',\n",
    "    'product': 'product',\n",
    "    'pastry_inventory': 'pastry_inventory',\n",
    "    'generations': 'generations',\n",
    "    'dates': 'dates',\n",
    "    'customer': 'customer'\n",
    "}\n",
    "\n",
    "# Specify the output directory containing the CSV files\n",
    "output_dir = r\"D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\Preprocessing Data\"\n",
    "\n",
    "# Specify the path for the SQLite database\n",
    "sqlite_file = r\"D:\\GitHub\\UEL-Course\\MLBA\\final_project\\Data\\database.sqlite\"\n",
    "\n",
    "# Call the function to convert each CSV to a table in the SQLite database\n",
    "csv_to_sqlite(dataframes_dict, output_dir, sqlite_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in c:\\users\\thanh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.30)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\thanh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sqlalchemy) (4.10.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\thanh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sqlalchemy) (3.0.3)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Engine' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table_name \u001b[38;5;129;01min\u001b[39;00m dataframes:\n\u001b[0;32m     30\u001b[0m     sqlite_sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- SQL for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m table\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 31\u001b[0m     sqlite_sql \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     sqlite_sql \u001b[38;5;241m=\u001b[39m sqlite_to_mysql(sqlite_sql) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Append data insert statements\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\sql.py:2513\u001b[0m, in \u001b[0;36mget_schema\u001b[1;34m(frame, name, keys, con, dtype, schema)\u001b[0m\n\u001b[0;32m   2490\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2491\u001b[0m \u001b[38;5;124;03mGet the SQL db table schema for the given frame.\u001b[39;00m\n\u001b[0;32m   2492\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2510\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.2.0\u001b[39;00m\n\u001b[0;32m   2511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2512\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con\u001b[38;5;241m=\u001b[39mcon) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m-> 2513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_sql_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\n\u001b[0;32m   2515\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\sql.py:2014\u001b[0m, in \u001b[0;36mSQLDatabase._create_sql_schema\u001b[1;34m(self, frame, table_name, keys, dtype, schema)\u001b[0m\n\u001b[0;32m   2006\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_sql_schema\u001b[39m(\n\u001b[0;32m   2007\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2008\u001b[0m     frame: DataFrame,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2012\u001b[0m     schema: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2013\u001b[0m ):\n\u001b[1;32m-> 2014\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43mSQLTable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2016\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2022\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(table\u001b[38;5;241m.\u001b[39msql_schema())\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\sql.py:899\u001b[0m, in \u001b[0;36mSQLTable.__init__\u001b[1;34m(self, name, pandas_sql_engine, frame, index, if_exists, prefix, index_label, schema, keys, dtype)\u001b[0m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m dtype\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;66;03m# We want to initialize based on a dataframe\u001b[39;00m\n\u001b[1;32m--> 899\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_table_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;66;03m# no data provided, read-only mode\u001b[39;00m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpd_sql\u001b[38;5;241m.\u001b[39mget_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\sql.py:1205\u001b[0m, in \u001b[0;36mSQLTable._create_table_setup\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m   1199\u001b[0m     Column,\n\u001b[0;32m   1200\u001b[0m     PrimaryKeyConstraint,\n\u001b[0;32m   1201\u001b[0m     Table,\n\u001b[0;32m   1202\u001b[0m )\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetaData\n\u001b[1;32m-> 1205\u001b[0m column_names_and_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_column_names_and_types\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sqlalchemy_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1207\u001b[0m columns: \u001b[38;5;28mlist\u001b[39m[Any] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1208\u001b[0m     Column(name, typ, index\u001b[38;5;241m=\u001b[39mis_index)\n\u001b[0;32m   1209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, typ, is_index \u001b[38;5;129;01min\u001b[39;00m column_names_and_types\n\u001b[0;32m   1210\u001b[0m ]\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\sql.py:1192\u001b[0m, in \u001b[0;36mSQLTable._get_column_names_and_types\u001b[1;34m(self, dtype_mapper)\u001b[0m\n\u001b[0;32m   1187\u001b[0m         idx_type \u001b[38;5;241m=\u001b[39m dtype_mapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_level_values(i))\n\u001b[0;32m   1188\u001b[0m         column_names_and_types\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28mstr\u001b[39m(idx_label), idx_type, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m   1190\u001b[0m column_names_and_types \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1191\u001b[0m     (\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39mcolumns[i]), dtype_mapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39miloc[:, i]), \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1192\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m))\n\u001b[0;32m   1193\u001b[0m ]\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m column_names_and_types\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Engine' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create a connection to a SQLite database in memory\n",
    "engine = create_engine('sqlite://', echo=False)\n",
    "\n",
    "# Assuming you have your DataFrames named like this\n",
    "dataframes = {\n",
    "    'staff': df_staff,\n",
    "    'sales_outlet': df_sales_outlet,\n",
    "    'sales_targets': df_sales_targets,\n",
    "    'product': df_product,\n",
    "    'pastry_inventory': df_pastry,\n",
    "    'customer': df_customer\n",
    "}\n",
    "\n",
    "# Export each DataFrame to the SQL database\n",
    "for table_name, df in dataframes.items():\n",
    "    df.to_sql(table_name, con=engine, index=False, if_exists='replace')\n",
    "\n",
    "# Function to convert SQLite SQL to MySQL SQL\n",
    "def sqlite_to_mysql(sqlite_sql):\n",
    "    mysql_sql = sqlite_sql.replace('INTEGER', 'INT').replace('TEXT', 'VARCHAR(255)')\n",
    "    return mysql_sql\n",
    "\n",
    "# Write the SQL dump to a file\n",
    "with open('mysql_dump.sql', 'w') as f:\n",
    "    for table_name in dataframes:\n",
    "        sqlite_sql = f\"\\n-- SQL for {table_name} table\\n\"\n",
    "        sqlite_sql += pd.io.sql.get_schema(engine, table_name, keys='id', con=engine)\n",
    "        sqlite_sql = sqlite_to_mysql(sqlite_sql) + \";\\n\\n\"\n",
    "        \n",
    "        # Append data insert statements\n",
    "        data_sql = ''\n",
    "        for _, row in dataframes[table_name].iterrows():\n",
    "            values = ', '.join([f\"'{item}'\" if isinstance(item, str) else str(item) for item in row])\n",
    "            data_sql += f\"INSERT INTO {table_name} VALUES ({values});\\n\"\n",
    "        \n",
    "        # Write to file\n",
    "        f.write(sqlite_sql + data_sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
